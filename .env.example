# RAGStrict 配置文件示例# RAGStrict Configuration Example# RAGStrict Configuration Example# RAGStrict Configuration# RAGStrict Configuration Example

# 复制此文件为 .ragstrict/.env 使用

# 复制此文件到 .ragstrict/.env 并根据需要修改

# ============================================

# 基础设置 - 保持默认即可# Copy this file to .env and modify as needed

# ============================================

# === 本地嵌入模型 ===

# 本地嵌入模型(用于文档向量化)

EMBEDDING_LOCAL_MODEL=sentence-transformers/all-MiniLM-L6-v2EMBEDDING_LOCAL_MODEL=sentence-transformers/all-MiniLM-L6-v2# Copy this file to .env and modify as needed# Copy this file to .env and modify as needed

EMBEDDING_DIMENSION=384

EMBEDDING_DIMENSION=384

# 文档分块大小

CHUNK_SIZE=512# === Local Models ===

CHUNK_OVERLAP=50

# === 文本分块参数 ===

# MCP服务器配置

MCP_PORT=3000CHUNK_SIZE=512# Sentence-transformers model for embeddings

MCP_HOST=localhost

CHUNK_OVERLAP=50

# 环境配置

ENVIRONMENT=developmentEMBEDDING_LOCAL_MODEL=sentence-transformers/all-MiniLM-L6-v2

DEBUG=false

# === MCP 服务器 ===

# 是否仅使用缓存的模型(不联网下载)

OFFLINE_MODE=falseMCP_PORT=3000EMBEDDING_DIMENSION=384# === API Configuration ===# === Network Mode ===



# ============================================MCP_HOST=localhost

# 说明

# ============================================

# 1. 以上配置保持默认即可正常使用

# 2. 如需AI对话功能,请配置 .env.api# === 环境设置 ===

# 3. 使用命令: rags config set enable_api true

ENVIRONMENT=development# === Chunking Parameters ===# Enable API calls (true/false)# internet: Use local sentence-transformers model (default)

DEBUG=false

CHUNK_SIZE=512

# === 离线模式 ===

# true: 仅使用已缓存的模型,不下载CHUNK_OVERLAP=50# If disabled, uses local models and simple chunking# intranet: Use Sangfor OneAPI for embedding and LLM chunking

# false: 允许下载模型

OFFLINE_MODE=false



# === API 配置 ===# === MCP Server ===ENABLE_API=falseNETWORK_MODE=internet

# API 配置在 .env.api 文件中(可选)

# 如果需要使用智能对话功能,请配置 .env.apiMCP_PORT=3000

# 使用命令: rags config set enable_api true

MCP_HOST=localhost



# === Environment ===# === Embedding API ===# === Embedding Settings ===

ENVIRONMENT=development

DEBUG=false# If ENABLE_API=true, uses this API for embeddings# For internet mode



# === Offline Mode ===# If API fails, falls back to local modelEMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Only use cached models, no downloads

OFFLINE_MODE=falseEMBEDDING_API_URL=https://oneapi.sangfor.com/v1/embeddingsEMBEDDING_DIMENSION=384



# === API Configuration ===EMBEDDING_API_MODEL=Qwen3-Embedding-8B

# API settings are in .env.api (optional)

# If .env.api exists and ENABLE_API=true, will use API with fallback to local modelsEMBEDDING_API_KEY=sk-your-api-key-here# For intranet mode - Sangfor OneAPI


# Embedding API

# Local embedding model (fallback or default when ENABLE_API=false)INTRANET_EMBEDDING_API=https://oneapi.sangfor.com/v1/embeddings

EMBEDDING_LOCAL_MODEL=sentence-transformers/all-MiniLM-L6-v2INTRANET_EMBEDDING_MODEL=Qwen3-Embedding-8B

EMBEDDING_DIMENSION=384INTRANET_API_KEY=sk-KskGcDMEQWGncNHr6bE2Ee61F22b40F8A1C09c8b150968Ff



# === LLM API ===# LLM API for intelligent chunking

# If ENABLE_API=true, uses this API for intelligent chunking and chatINTRANET_LLM_API=https://oneapi.sangfor.com/v1/chat/completions

# If API fails, falls back to simple chunkingINTRANET_LLM_MODEL=qwen3-32b

LLM_API_URL=https://oneapi.sangfor.com/v1/chat/completions

LLM_API_MODEL=qwen3-32b# === Chunking Parameters ===

LLM_API_KEY=sk-your-api-key-hereCHUNK_SIZE=512

CHUNK_OVERLAP=50

# === Chunking Parameters ===

CHUNK_SIZE=512# === MCP Server ===

CHUNK_OVERLAP=50MCP_PORT=3000

MCP_HOST=localhost

# === MCP Server ===

MCP_PORT=3000# === Environment ===

MCP_HOST=localhostENVIRONMENT=development

DEBUG=false

# === Environment ===

ENVIRONMENT=development# === Offline Mode ===

DEBUG=false# Only for internet mode: use cached models only, no downloads

OFFLINE_MODE=false

# === Offline Mode ===
# Only use cached models, no downloads
OFFLINE_MODE=false
